\section{Methods}
\subsection{Experts}
For the heuristic evaluation, five experts were selected. All of them are first-year Computer Science and Engineering students at TU Delft and took part in a Heuristics Evaluations course, taught by an expert in the field. Furthermore, at the time of questioning, all selected students had been working on the same project for a month. Therefore, all of them had seen various versions of the design that they had to evaluate.

Due to the aforementioned circumstances, the selected students qualify as a suitable choice for this evaluation. Furthermore, they demonstrate an adequate level of expertise in the field of web-page usability.

\subsection{Procedure}
The following procedure was followed for each of the five evaluators. A single observer, which is part of the developing team, was assigned to each evaluator. The expert was given a short presentation of the way the evaluation will be handled. They were told that they will be given only the non-functional design of the application and were presented with the list of heuristics that they would need to assess the prototype against. 

Here we provide the heuristics that were presented to each evaluator. They are the heuristics used by Nielsen \cite{nielsen1994heuristic}.
\begin{enumerate}
  \item Visibility of system status;
  \item Match between system and the real world;
  \item User control and freedom;
  \item Consistency and standards;
  \item Error prevention;
  \item Recognition rather than recall;
  \item Flexibility and efficiency of use;
  \item Aesthetic and minimalist design;
  \item Help users recognize, diagnose, and recover from errors;
  \item Help and documentation.
\end{enumerate}

Evaluators were presented with the format which they would use in order to report their findings. We present this format in a later subsection of the report.

Each expert was interacting with the prototype from inside of the "Figma" project. They were only analyzing images, which had no functionality. Therefore, the role of the observer was to walk the evaluator through the flow of the application. 
This interaction between evaluator and observer allowed for more flexibility, as an action that would be difficult to show through images can be verbally explained by the observer, instead of showing which scene would be loaded.

At first, the evaluators were instructed to briefly pass through the interface, in order to get familiar with it. During this first glance, some problems would already be identified and written down, but that was not the main goal of this interaction.

After that, the experts were instructed to be more rigorous and start looking for problems in each scene separately. They were instructed to first look at the big image and look for basic errors and then get into the details of each scene - separate buttons, text fields, etc. Each time an inconsistency against the provided heuristics was found, the evaluator would describe the problem to the observer, and the observer would note it down in the format that had been discussed earlier.

Depending on the evaluator, the whole process would last from 45 minutes to 1 hour and 30 minutes.

\subsection{Measures (Data collection)}
For each of the problems found, the evaluators were supposed to name the following:
\begin{enumerate}
  \item Problem description
  \item Likely/actual difficulties
  \item Context of the problem
  \item Assumed causes
\end{enumerate}

This specific list is used in order to more easily identify when two experts are reporting on the same problem. It was introduced by Cockton, Woolrych, Hall and Hindmarch

Before the meeting with an expert, each observer had prepared an empty table. The columns of that table were exactly the 4 items in the list. And for each new problem an expert found, a new row was inserted into the table.

After all experts were questioned, a document with all tables was compiled. That was exactly the raw data that was used for this heuristic evaluation.